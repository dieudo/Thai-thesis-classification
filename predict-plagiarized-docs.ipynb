{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from utils import pretty_trim, simple_split, score_top_preds, get_cmap\n",
    "from collections import Counter\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "import chardet\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Loading trained models **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = 'models_persistence/pickle_models'\n",
    "(pretty_trim, counter, tfidf, rfe, clfs) = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "str_labels = [u'0 บริหารธุรกิจ',\n",
    "    u'1 ประมง',\n",
    "    u'2 มนุษยศาสตร์',\n",
    "    u'3 วนศาสตร์',\n",
    "    u'4 วิทยาการจัดการ',\n",
    "    u'5 วิทยาศาสตร์',\n",
    "    u'6 วิทยาศาสตร์การกีฬา',\n",
    "    u'7 วิศวกรรมศาสตร์',\n",
    "    u'8 ศิลปศาสตร์และวิทยาศาสตร์',\n",
    "    u'9 ศึกษาศาสตร์',\n",
    "    u'10 ศึกษาศาสตร์และพัฒนศาสตร์',\n",
    "    u'11 สถาปัตยกรรมศาสตร์',\n",
    "    u'12 สังคมศาสตร์',\n",
    "    u'13 สัตวแพทยศาสตร์',\n",
    "    u'14 สิ่งแวดล้อม',\n",
    "    u'15 อุตสาหกรรมเกษตร',\n",
    "    u'16 เกษตร',\n",
    "    u'17 เศรษฐศาสตร์',\n",
    "    u'18 โครงการจัดตั้งวิทยาเขตสุพรรณบุรี',\n",
    "    u'19 โครงการสหวิทยาการระดับบัณฑิตศึกษา']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Read segmented docs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files: 2165\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_path = u'./corpus/segmented-journal' # must be a segmented doc path\n",
    "dataset_contents = []\n",
    "filename2index = dict()\n",
    "for i, filename in enumerate(os.listdir(doc_path)):\n",
    "    path = os.path.join(doc_path, filename)\n",
    "    filename2index[filename] = i\n",
    "    with open(path) as f:\n",
    "        content = f.read()\n",
    "#         if chardet.detect(content)['encoding'] == 'ascii':\n",
    "#             continue\n",
    "        content = content.decode('utf8')\n",
    "        dataset_contents.append(content)\n",
    "print 'total files:', len(dataset_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply learning pipeline to all the docs\n",
    "First trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in xrange(len(dataset_contents)):\n",
    "    dataset_contents[i] = pretty_trim(dataset_contents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then count words and apply Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.4 s\n",
      "Wall time: 181 ms\n",
      "(2165, 250000)\n"
     ]
    }
   ],
   "source": [
    "%time X_new_count = counter.transform(dataset_contents)\n",
    "%time X_new_tfidf = tfidf.transform(X_new_count)\n",
    "print X_new_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 61 ms\n",
      "(2165, 20000)\n"
     ]
    }
   ],
   "source": [
    "%time X_new_rfe = rfe.transform(X_new_tfidf)\n",
    "print X_new_rfe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 1,\n",
       "         2: 1,\n",
       "         3: 1,\n",
       "         5: 1322,\n",
       "         7: 15,\n",
       "         9: 32,\n",
       "         12: 2,\n",
       "         15: 192,\n",
       "         16: 575,\n",
       "         17: 17,\n",
       "         19: 7})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_new_rfe)\n",
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_orig = X_new_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize approximated labels using heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [word -> class_label] mapping dictioanry\n",
    "approx_label = {\n",
    "#     \"liber\": 8,\n",
    "#     \"art\": 8,\n",
    "    \"agricultur\": 16,\n",
    "    \"agro\": 16,\n",
    "    \"educ\": 9,\n",
    "#     \"social\": 12,\n",
    "#     \"fisheri\": 1,\n",
    "#     \"manag\": 4,\n",
    "    \"scienc\": 5,\n",
    "    \"technolog\": 5,\n",
    "#     \"medicin\": 13,\n",
    "#     \"pharmaci\": 13,\n",
    "#     \"forestri\": 3,\n",
    "#     \"forest\": 3,\n",
    "    \"engin\": 7,\n",
    "    \"econom\": 17,\n",
    "    \"architectur\": 11,\n",
    "#     \"human\": 2,\n",
    "    \"biotechnolog\": 5,\n",
    "#     \"environment\": 14,\n",
    "#     \"environ\": 14,\n",
    "#     \"veterinari\": 13,\n",
    "#     \"busi\": 0,\n",
    "#     u\"ธุรกิจ\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find approximated labels by searching for the faculty name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_heuristic_y(approx_label):\n",
    "    heuristic_y = np.zeros(len(dataset_contents), dtype=np.int32) - 1 # starts with -1 filled\n",
    "    for ci in range(len(dataset_contents)):\n",
    "        words = dataset_contents[ci].split()\n",
    "        contexts = []\n",
    "        wis = []\n",
    "        for wi, word in enumerate(words):\n",
    "            if u'faculti' in word or u'คณะ' in word:\n",
    "                context = words[wi-3:wi+5]\n",
    "                contexts.append(context)\n",
    "                wis.append(wi)\n",
    "                for w in context:\n",
    "                    if w in approx_label:\n",
    "                        heuristic_y[ci] = approx_label[w]\n",
    "                        break\n",
    "            if heuristic_y[ci] != -1:\n",
    "                break\n",
    "        if contexts: # logging\n",
    "            label = str_labels[heuristic_y[ci]] if heuristic_y[ci] != -1 else 'UNKNOWN'\n",
    "#             print 'Document No.', ci, '(', label, ')'\n",
    "\n",
    "            for i in range(len(contexts)):\n",
    "#                 print 'Word No.', wis[i], ' => ',\n",
    "                for w in contexts[i]:\n",
    "                    if w in approx_label:\n",
    "                        w = '[%s]' % w\n",
    "#                     print w,\n",
    "#                 print\n",
    "    return heuristic_y, heuristic_y != -1 # test data that do not have approximated label would be invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label Approximations: 1592\n"
     ]
    }
   ],
   "source": [
    "heuristic_y, valid_mask = find_heuristic_y(approx_label)\n",
    "print 'Total Label Approximations:', np.count_nonzero(valid_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate accuracy score on approximated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.564070351759\n",
      "Counter({5: 845, 16: 462, 7: 133, 9: 89, 17: 56, 11: 7})\n",
      "Counter({5: 955, 16: 408, 15: 169, 9: 25, 17: 14, 7: 12, 19: 6, 0: 1, 3: 1, 12: 1})\n"
     ]
    }
   ],
   "source": [
    "print 'Accuracy:', accuracy_score(heuristic_y[valid_mask], y_pred[valid_mask])\n",
    "print Counter(heuristic_y[valid_mask])\n",
    "print Counter(y_pred[valid_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2165\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "n = len(dataset_contents)\n",
    "print n\n",
    "# choose a doc that have heuristic_y only\n",
    "def choose_random_doc():\n",
    "    while True:\n",
    "        doc_id = np.random.choice(n)\n",
    "        if valid_mask[doc_id]:\n",
    "            return doc_id\n",
    "\n",
    "# return 2 distinct documents, (the labels are different)\n",
    "def choose_distinct_docs():\n",
    "    while True:\n",
    "        doc1 = choose_random_doc()\n",
    "        doc2 = choose_random_doc()\n",
    "        if heuristic_y[doc1] != heuristic_y[doc2]:\n",
    "            return doc1, doc2\n",
    "\n",
    "def segment(doc, wordbegin, wordend):\n",
    "    return dataset_contents[doc].split()[wordbegin:wordend]\n",
    "        \n",
    "# input doc1 and doc2 are indices\n",
    "def mix_docs(doc1, doc2):\n",
    "    # TODO: change these magic numbers into something less deterministic\n",
    "    seg1 = segment(doc1, 200 + np.random.randint(50), 250 + np.random.randint(50))\n",
    "    seg2 = segment(doc2, 200 + np.random.randint(50), 250 + np.random.randint(50))\n",
    "    seg3 = segment(doc1, 600 + np.random.randint(50), 650 + np.random.randint(50))\n",
    "    seg4 = segment(doc2, 600 + np.random.randint(50), 650 + np.random.randint(50))\n",
    "    mixed = seg1 + seg2 + seg3 + seg4\n",
    "    return ' '.join(mixed)\n",
    "\n",
    "def gen_plagiarized_contents(total):\n",
    "    contents = []\n",
    "    ys = []\n",
    "    source_docs = []\n",
    "    for i in range(total):\n",
    "        doc1, doc2 = choose_distinct_docs()\n",
    "        new_content = mix_docs(doc1, doc2)\n",
    "        new_y = [heuristic_y[doc1], heuristic_y[doc2]]\n",
    "        contents.append(new_content)\n",
    "        ys.append(new_y)\n",
    "        source_docs.append([doc1, doc2])\n",
    "    return contents, ys, source_docs\n",
    "\n",
    "def vectorize_contents(contents):\n",
    "    X_new_count = counter.transform(contents)\n",
    "    X_new_tfidf = tfidf.transform(X_new_count)\n",
    "    X_new_rfe = rfe.transform(X_new_tfidf)\n",
    "    return X_new_rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_gen = 1000\n",
    "plagiarized_docs, plagiarized_labels, source_docs = gen_plagiarized_contents(total_gen)\n",
    "X_plagiarized = vectorize_contents(plagiarized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[7, 16],\n",
       "  [7, 16],\n",
       "  [5, 17],\n",
       "  [7, 5],\n",
       "  [5, 17],\n",
       "  [5, 16],\n",
       "  [5, 16],\n",
       "  [16, 5],\n",
       "  [5, 16],\n",
       "  [5, 16]],\n",
       " [[1095, 466],\n",
       "  [1515, 1215],\n",
       "  [1082, 2047],\n",
       "  [2068, 600],\n",
       "  [1129, 1500],\n",
       "  [815, 455],\n",
       "  [1076, 791],\n",
       "  [1367, 1152],\n",
       "  [200, 1863],\n",
       "  [1895, 1570]],\n",
       " 7,\n",
       " 16)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing examples of plagiarized labels for some documents\n",
    "plagiarized_labels[:10], source_docs[:10], heuristic_y[source_docs[0][0]], heuristic_y[source_docs[0][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing accuracy of plagiarism detection\n",
    "We are finding whether or not any predicted class is within the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(X_plagiarized)\n",
    "# k is number of predicted labels you want\n",
    "def get_predicted_labels(probs, k):\n",
    "    # sort the probabilities ascendingly then take the last k probs' indices\n",
    "    return probs.argsort(axis=1)[:,-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 16] [16 15  5] True\n"
     ]
    }
   ],
   "source": [
    "# testing the mechanics of how we interpret valid match\n",
    "pred_labels = get_predicted_labels(probs, 3)\n",
    "i = 0 # index of the content we want to test\n",
    "print plagiarized_labels[i], pred_labels[i], any(y in plagiarized_labels[i] for y in pred_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define a function to find the accuracy given number of predicted labels\n",
    "def evaluate_accuracy(k):\n",
    "    pred_labels = get_predicted_labels(probs, k)\n",
    "    # a list of boolean values showing whether we predict correctly or not\n",
    "    accurate_predictions = [any(y in plagiarized_labels[i] for y in pred_labels[i]) for i in range(total_gen)]\n",
    "    return np.mean(accurate_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total plagiarized docs: 1000\n",
      "Accuracy given k=1: 0.807\n",
      "Accuracy given k=2: 0.957\n",
      "Accuracy given k=3: 0.989\n",
      "Accuracy given k=4: 0.996\n",
      "Accuracy given k=5: 1.0\n",
      "Accuracy given k=6: 1.0\n",
      "Accuracy given k=7: 1.0\n",
      "Accuracy given k=8: 1.0\n",
      "Accuracy given k=9: 1.0\n",
      "Accuracy given k=10: 1.0\n"
     ]
    }
   ],
   "source": [
    "print 'Total plagiarized docs:', total_gen\n",
    "# k can be at most 20 because there are 20 classes\n",
    "for k in range(1, 11):\n",
    "    print 'Accuracy given k=%d:' % k, evaluate_accuracy(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "from collections import Counter\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'models_persistence/final_dataset'\n",
    "X_train_final, y_train, X_test_final, y_test = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_clusters = len(str_labels)\n",
    "km = KMeans(n_clusters=n_clusters, max_iter=10, n_init=10)\n",
    "pred = km.fit_predict(X_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Model Evaluation Metrics ====\n",
      "Homogeneity: 0.341512505578\n",
      "Completeness: 0.350862947214\n",
      "V-measure: 0.346124588139\n",
      "Predictions:\n",
      "0 6\n",
      "1 81\n",
      "2 53\n",
      "3 84\n",
      "4 258\n",
      "5 42\n",
      "6 52\n",
      "7 39\n",
      "8 597\n",
      "9 19\n",
      "10 186\n",
      "11 40\n",
      "12 22\n",
      "13 63\n",
      "14 131\n",
      "15 70\n",
      "16 170\n",
      "17 7\n",
      "18 52\n",
      "19 67\n"
     ]
    }
   ],
   "source": [
    "#%% Clustering Model Evaluation Metrics\n",
    "homogeneity, completeness, v_measure = homogeneity_completeness_v_measure(y_train, pred)\n",
    "print '==== Model Evaluation Metrics ===='\n",
    "print 'Homogeneity:', homogeneity\n",
    "print 'Completeness:', completeness\n",
    "print 'V-measure:', v_measure\n",
    "print 'Predictions:'\n",
    "for key, value in Counter(pred).iteritems():\n",
    "    print key, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({4: 735, 8: 265}),\n",
       " Counter({1: 2, 4: 1941, 7: 1, 8: 198, 10: 7, 12: 5, 13: 3, 14: 4, 19: 4}))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_plagiarized_cluster = km.predict(X_plagiarized)\n",
    "y_orig_cluster = km.predict(X_orig)\n",
    "Counter(y_plagiarized_cluster), Counter(y_orig_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 4 2\n",
      "4 4 4 2\n",
      "8 4 14 0\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "8 4 4 0\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "8 4 4 0\n",
      "8 4 8 1\n",
      "8 4 8 1\n",
      "4 4 4 2\n",
      "8 4 4 0\n",
      "8 4 4 0\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "4 4 4 2\n",
      "Average Equal Counts: 1.558\n",
      "equal_count=0 counts: 123\n",
      "equal_count=1 counts: 196\n",
      "equal_count=2 counts: 681\n"
     ]
    }
   ],
   "source": [
    "# example: check if the original docs are in the same cluster or not\n",
    "equal_counts = []\n",
    "show_num = 20\n",
    "for i in range(total_gen):\n",
    "    doc1_cluster = y_orig_cluster[source_docs[i][0]]\n",
    "    doc2_cluster = y_orig_cluster[source_docs[i][1]]\n",
    "    equal_count = 0\n",
    "    equal_count += int(y_plagiarized_cluster[i] == doc1_cluster)\n",
    "    equal_count += int(y_plagiarized_cluster[i] == doc2_cluster)\n",
    "    equal_counts.append(equal_count)\n",
    "    if i < show_num:\n",
    "        print y_plagiarized_cluster[i], doc1_cluster, doc2_cluster, equal_count\n",
    "equal_counts = np.array(equal_counts)\n",
    "print 'Average Equal Counts:', equal_counts.mean()\n",
    "for i in range(3):\n",
    "    print 'equal_count=%d counts:' % i, (equal_counts==i).sum()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
